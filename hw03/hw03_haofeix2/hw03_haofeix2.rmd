---
title: 'STAT 420: Homework 03'
author: "Haofei Xu, haofeix2"
output:
  html_document:
    theme: readable
    toc: yes
---

## Exercise 1 (Using `lm`)
**(a) Answer: ** 
```{r}
faithful_model = lm(eruptions ~ waiting, data = faithful)
summary(faithful_model)
```

**(b)** Output only the estimated regression coefficients. Interpret $\beta_0$ and $\hat{\beta_1}$ in the *context of the problem*. Be aware that only one of those is an estimate.
```{r}
coef(faithful_model)
```
**Conclusion: ** 

1. The intercept parameter $\beta_0$ tells us the **mean** eruption time when waiting time is zero.
2. The **estimated** slope parameter $\hat{\beta_1}$ tells us that for an increase in waiting time of one minute, the **estimated mean** eruption time increases by $\hat{\beta_1}$ = 0.07562795.

**(c)** Use your model to predict the duration of an eruption based on a waiting time of **80** minutes. Do you feel confident in this prediction? Briefly explain.
```{r}
unique(faithful$waiting) # 80 is in this set of data
predict(faithful_model, data.frame(waiting = 80))
```
**Explanation: ** Since 80 is an observed value of waiting time, we are confident with this prediction.

**(d)** Use your model to predict the duration of an eruption based on a waiting time of **120** minutes. Do you feel confident in this prediction? Briefly explain.
```{r}
120 %in% unique(faithful$waiting) # NOT an observed data
min(faithful$waiting) < 120 & 120 < max(faithful$waiting) # NOT even in the range
predict(faithful_model, data.frame(waiting = 120))
```
**Explanation: ** Since 120 is neither an observed value of waiting time nor in the range of the datasets. This is considered extrapolation and we are less confident with this prediction.

**(e)** Calculate the RSS for this model.
```{r}
RSS = sum(resid(faithful_model) ^ 2)
RSS
```

**(f)** Create a scatterplot of the data and add the fitted regression line. Make sure your plot is well labeled and is somewhat visually appealing.
```{r}
plot(eruptions ~ waiting, data = faithful,
     xlab = "Waiting time to next eruption (in mins)",
     ylab = "Eruption time in mins",
     main = "Eruption Length vs Waiting Time",
     pch  = 20,
     cex  = 1,
     col  = "orange")
abline(faithful_model, lwd = 2)
```

**(g)** Report the value of $R^2$ for the model. Do so directly. Do not simply copy and paste the value from the full output in the console after running `summary()` in part **(a)**.
```{r}
summary(faithful_model)$r.squared
```

## Exercise 2 (Writing Functions)

This exercise is a continuation of Exercise 1.

**(a)** Write a function called `get_sd_est` that calculates an estimate of $\sigma$ in one of two ways depending on input to the function. The function should take two arguments as input:

- `model_resid` - A vector of residual values from a fitted model.
- `mle` - A logical (`TRUE` / `FALSE`) variable which defaults to `FALSE`.

The function should return a single value:

- $s_e$ if `mle` is set to `FALSE`.
- $\hat{\sigma}$ if `mle` is set to `TRUE`.
```{r}
get_sd_est = function(model_resid, mle = FALSE) {
  n = ifelse(mle, length(model_resid), length(model_resid) - 2)
  sqrt(sum(model_resid ^ 2) / n)
}
```

**(b)** Run the function `get_sd_est` on the residuals from the model in Exercise 1, with `mle` set to `FALSE`.
```{r}
get_sd_est(resid(faithful_model), FALSE)
```

**(c)** Run the function `get_sd_est` on the residuals from the model in Exercise 1, with `mle` set to `TRUE`.
```{r}
get_sd_est(resid(faithful_model), TRUE)
```

**(d)** To check your work, output `summary(faithful_model)$sigma`. It should match at least one of **(b)** or **(c)**.
```{r}
summary(faithful_model)$sigma
```
## Exercise 3 (Simulating SLR)

Consider the model

\[
Y_i = 3 - 7 x_i + \epsilon_i
\]

with 

\[
\epsilon_i \sim N(\mu = 0, \sigma^2 = 4)
\]

where $\beta_0 = 3$ and $\beta_1 = -7$.

Before answering the following parts, set a seed value equal to **your** birthday, as was done in the previous assignment.

```{r}
birthday = 20000623
set.seed(birthday)
```

**(a)** Use `R` to simulate `n = 50` observations from the above model. For the remainder of this exercise, use the following "known" values of $x$.

```{r}
sim_slr = function(x, beta_0, beta_1, sigma) {
  n = length(x)
  epsilon = rnorm(n, mean = 0, sd = sigma)
  y = beta_0 + beta_1 * x + epsilon
  data.frame(predictor = x, response = y)
}
x = runif(n = 50, 0, 10)
sim_data = sim_slr(x, 3, -7, 2)
```

**(b)** Fit a model to your simulated data. Report the estimated coefficients. Are they close to what you would expect? Briefly explain.
```{r}
model = lm(response ~ predictor, data = sim_data)
coef(model)
```
**Explanation: ** It's close to the $\beta_0 = 3$ and $\beta_1 = -7$ provided.

**(c)** Plot the data you simulated in part **(a)**. Add the regression line from part **(b)**. Hint: Keep the two commands in the same chunk, so `R` knows what plot to add the line to when knitting your `.Rmd` file.
```{r}
plot(response ~ predictor, data = sim_data,     
     xlab = "Simulated Predictor Variable",
     ylab = "Simulated Response Variable",
     main = "Simulated Regression Data",
     pch  = 20,
     cex  = 2,
     col  = "grey")
abline(model, lwd = 2, col = "darkorange")
```

**(d)** Use `R` to repeat the process of simulating `n = 50` observations from the above model $2000$ times. Each time fit a SLR model to the data and store the value of $\hat{\beta_1}$ in a variable called `beta_hat_1`. Some hints:

- Use a `for` loop.
- Create `beta_hat_1` before writing the `for` loop. Make it a vector of length $2000$ where each element is `0`.
- Inside the body of the `for` loop, simulate new $y$ data each time. Use a variable to temporarily store this data together with the known $x$ data as a data frame.
- After simulating the data, use `lm()` to fit a regression. Use a variable to temporarily store this output.
- Use the `coef()` function and `[]` to extract the correct estimated coefficient.
- Use `beta_hat_1[i]` to store in elements of `beta_hat_1`.
- See the notes on [Distribution of a Sample Mean](http://daviddalpiaz.github.io/appliedstats/introduction-to-r.html#distribution-of-a-sample-mean) for some inspiration.

You can do this differently if you like. Use of these hints is not required.
```{r}
beta_hat_1 = rep(0, 2000)
for(i in 1:2000){
  sim_data = sim_slr(x, 3, -7, 2)  
  model = lm(response ~ predictor, data = sim_data)  
  beta_hat_1[i] = coef(model)[2]
  }
```
**(e)** Report the mean and standard deviation of `beta_hat_1`. Do either of these look familiar?
```{r}
mean(beta_hat_1)
sd(beta_hat_1)
```
**Mean of `beta_hat_1` looks similar to $\beta_1 = -7$.**

**(f)** Plot a histogram of `beta_hat_1`. Comment on the shape of this histogram.
```{r}
hist(beta_hat_1,
     breaks = 20,
     xlab = expression(hat(beta)[1]))
```

**Comments: ** It looks like a normal distribution curve with a mean near -7.0.

## Exercise 4 (Be a Skeptic)

Consider the model

\[
Y_i = 10 + 0 x_i + \epsilon_i
\]

with

\[
\epsilon_i \sim N(\mu = 0, \sigma^2 = 1)
\]

where $\beta_0 = 10$ and $\beta_1 = 0$.

Before answering the following parts, set a seed value equal to **your** birthday, as was done in the previous assignment.

```{r}
birthday = 20000623
set.seed(birthday)
```

**(a)** Use `R` to repeat the process of simulating `n = 25` observations from the above model $1500$ times. For the remainder of this exercise, use the following "known" values of $x$.

```{r}
x = runif(n = 25, 0, 10)
```

Each time fit a SLR model to the data and store the value of $\hat{\beta_1}$ in a variable called `beta_hat_1`. You may use [the `sim_slr ` function provided in the text](http://daviddalpiaz.github.io/appliedstats/simple-linear-regression.html#simulating-slr). Hint: Yes $\beta_1 = 0$.
```{r}
beta_hat_1 = rep(0, 1500)
for(i in 1:1500) {
  sim_data = sim_slr(x, 10, 0, 1) 
  model = lm(response ~ predictor, data = sim_data)  
  beta_hat_1[i] = coef(model)[2]
  }
```

**(b)** Plot a histogram of `beta_hat_1`. Comment on the shape of this histogram.
```{r}
hist(beta_hat_1,
     breaks = 20,
     xlab   = expression(hat(beta)[1])
     )
```

**Comments: ** It looks like a normal distribution curve with a mean near 0.0.

**(c)** Import the data in [`skeptic.csv`](skeptic.csv) and fit a SLR model. The variable names in `skeptic.csv` follow the same convention as those returned by `sim_slr()`. Extract the fitted coefficient for $\beta_1$.
```{r}
skeptic = read.csv("skeptic.csv")
SLR_model = lm(response ~ predictor, data = skeptic)
coef(SLR_model)[2]
```

**(d)** Re-plot the histogram from **(b)**. Now add a vertical red line at the value of $\hat{\beta_1}$ in part **(c)**. To do so, you'll need to use `abline(v = c, col = "red")` where `c` is your value.
```{r}
hist(beta_hat_1,
     xlab   = expression(hat(beta)[1]),
     breaks  = 20,
     col    = "darkorange")
abline(v = coef(SLR_model)[2], col = "red", lwd = 2)
```

**(e)** Your value of $\hat{\beta_1}$ in **(c)** should be positive. What proportion of the `beta_hat_1` values are larger than your $\hat{\beta_1}$? Return this proportion, as well as this proportion multiplied by `2`.
```{r}
mean(beta_hat_1 > coef(SLR_model)[2])
mean(beta_hat_1 > coef(SLR_model)[2]) * 2
```

**(f)** Based on your histogram and part **(e)**, do you think the [`skeptic.csv`](skeptic.csv) data could have been generated by the model given above? Briefly explain.
```{r}
range(beta_hat_1)
```
**Explanation: ** Since 0.154081 is in the range of $\hat{\beta_1}$, it is possible that it could be generated. **However,** from (e), only about 1% (data varies) of the data is larger than the model, it is not highly possible.

## Exercise 5 (Comparing Models)

For this exercise we will use the data stored in [`goalies.csv`](goalies.csv). It contains career data for all 716 players in the history of the National Hockey League to play goaltender through the 2014-2015 season. The variables in the dataset are:

- `Player` - NHL Player Name
- `First` - First year of NHL career
- `Last` - Last year of NHL career
- `GP` - Games Played
- `GS` - Games Started
- `W` - Wins
- `L` - Losses
- `TOL` - Ties/Overtime/Shootout Losses
- `GA` - Goals Against
- `SA` - Shots Against
- `SV` - Saves
- `SV_PCT` - Save Percentage
- `GAA` - Goals Against Average
- `SO` - Shutouts
- `MIN` - Minutes
- `G` - Goals (that the player recorded, not opponents)
- `A` - Assists (that the player recorded, not opponents)
- `PTS` - Points (that the player recorded, not opponents)
- `PIM` - Penalties in Minutes

For this exercise we will define the "Root Mean Square Error" of a model as

\[
RMSE = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}.
\]

**(a)** Fit a model with "wins"" as the response and "minutes" as the predictor. Calculate the RMSE of this model. Also provide a scatterplot with the fitted regression line.
```{r}
goalies = read.csv("goalies.csv")
model_a = lm(W ~ MIN, data = goalies)
RMSE = sqrt(mean(resid(model_a) ^ 2))
RMSE
plot(W ~ MIN, data = goalies,
     xlab = "Minutes",
     ylab = "Wins",
     main = "Wins vs Minutes",
     pch  = 20,
     cex  = 1,
     col  = "darkorange")
abline(model_a, lwd = 2)
```

**(b)** Fit a model with "wins"" as the response and "goals against" as the predictor. Calculate the RMSE of this model. Also provide a scatterplot with the fitted regression line.
```{r}
model_b = lm(W ~ GA, data = goalies)
RMSE = sqrt(mean(resid(model_b) ^ 2))
RMSE
plot(W ~ GA, data = goalies,
     xlab = "Goals Against",
     ylab = "Wins",
     main = "Wins vs Goals Against",
     pch  = 20,
     cex  = 1,
     col  = "darkorange")
abline(model_b, lwd = 2)
```

**(c)** Fit a model with "wins"" as the response and "shutouts" as the predictor. Calculate the RMSE of this model. Also provide a scatterplot with the fitted regression line.
```{r}
model_c = lm(W ~ SO, data = goalies)
RMSE = sqrt(mean(resid(model_c) ^ 2))
RMSE
plot(W ~ SO, data = goalies,
     xlab = "Shutouts",
     ylab = "Wins",
     main = "Wins vs Shutouts",
     pch  = 20,
     cex  = 1,
     col  = "darkorange")
abline(model_c, lwd = 2)
```

**(d)** Based on the previous three models, which of the three predictors used is most helpful for predicting wins? Briefly explain.

**Explanation: ** The model_a in part(a), using "minutes" as the predictor, is the best predictor since it has the least RMSE.